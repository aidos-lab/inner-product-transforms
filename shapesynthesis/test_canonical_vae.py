"""
Custom script to evaluate the results generated by our comparison partner. 
The results are in the ./results/canconicalvae folder. 

Link: https://github.com/AnjieCheng/CanonicalVAE/tree/main?tab=readme-ov-file
"""

import json
import os
from typing import Any
import numpy as np
import torch
from metrics.evaluation import EMD_CD

final_results = []

cate = "airplane"
run = 1

for cate in ["airplane", "car", "chair"]:
    for run in range(5):
        ref_path = f"./results/canonicalvae/{cate}_ref_{run}.npy"
        rec_path = f"./results/canonicalvae/{cate}_recon_{run}.npy"

        if os.path.exists(ref_path) and os.path.exists(rec_path):
            ref_pcs = torch.tensor(np.load(ref_path))
            rec_pcs = torch.tensor(np.load(rec_path))

            results: dict[str, Any] = EMD_CD(
                rec_pcs,
                ref_pcs,
                batch_size=8,
                reduced=True,
                accelerated_cd=True,
            )

            results = {
                ("%s" % k): (v if isinstance(v, float) else v.item())
                for k, v in results.items()
            }
            results["model"] = f"canonicalvae_{cate}"
            results["normalized"] = False
            final_results.append(results)

with open("./results/canonicalvae/results.json", "w", encoding="utf-8") as f:
    json.dump(final_results, f)
